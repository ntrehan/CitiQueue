{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTmc1gcAmKdq",
        "outputId": "3659d05e-f979-4b0d-fa12-c4b6bbf3ac28"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=50a8439e19a568603099d7c44e5e542d3f42ecf73852cb8b8a5da8978a0b938d\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DuSn3YcmpwD",
        "outputId": "9ee91b2d-3f0e-45ea-fc47-ffab7e0d7234"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import pytz"
      ],
      "metadata": {
        "id": "UMWepRMQ_cuS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_file_path = '/gdrive/My Drive/trip_model.pkl'\n",
        "scaler_file_path = '/gdrive/My Drive/scaler_model.pkl'\n",
        "data_file_path = '/gdrive/My Drive/station_data.json'\n",
        "output_file_path_pred = '/gdrive/My Drive/station_data.json'\n",
        "output_file_path_spark = \"/gdrive/My Drive/output_priority.json\""
      ],
      "metadata": {
        "id": "T08P2EJ7Dq9X"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_current_datetime():\n",
        "    eastern = pytz.timezone('US/Eastern')\n",
        "    return datetime.now(eastern).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "eastern = pytz.timezone('US/Eastern')"
      ],
      "metadata": {
        "id": "LDljfbbwHAmc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(file_path):\n",
        "    with open(file_path, 'rb') as model_file:\n",
        "        return pickle.load(model_file)\n",
        "\n",
        "def load_scaler(file_path):\n",
        "    with open(file_path, 'rb') as scaler_file:\n",
        "        return pickle.load(scaler_file)\n",
        "\n",
        "def load_station_data(file_path):\n",
        "    with open(file_path, 'r') as data_file:\n",
        "        return json.load(data_file)\n",
        "\n",
        "def scale_features(data_point, scaler):\n",
        "    return scaler.transform([data_point])[0]\n",
        "\n",
        "def predict_traffic(model, scaled_data_point):\n",
        "    return model.predict([scaled_data_point])[0]\n",
        "\n",
        "\n",
        "def add_predictions_to_data(station_data, model, scaler):\n",
        "    # Convert the station_data list of dictionaries to a pandas DataFrame\n",
        "    df = pd.DataFrame(station_data)\n",
        "\n",
        "    # Extract features into a separate DataFrame and rename columns\n",
        "    features = df[['day_of_week', 'hour', 'local_id']]\n",
        "    features = features.rename(columns={'day_of_week': 'day', 'hour': 'hour', 'local_id': 'station_id'})\n",
        "\n",
        "    # Scale the features using the loaded scaler\n",
        "    scaled_features = scaler.transform(features)\n",
        "\n",
        "    # Use the trained model to predict the target variable\n",
        "    df['predicted_traffic'] = model.predict(scaled_features)\n",
        "\n",
        "    # Convert the DataFrame back to a list of dictionaries\n",
        "    updated_station_data = df.to_dict('records')\n",
        "\n",
        "    return updated_station_data\n",
        "\n",
        "\n",
        "def save_data_with_predictions(station_data, output_file_path):\n",
        "    with open(output_file_path, 'w') as output_file:\n",
        "        json.dump(station_data, output_file)"
      ],
      "metadata": {
        "id": "aOkR0mQ4_fId"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def runPrediction():\n",
        "    rf_model = load_model(model_file_path)\n",
        "    scaler = load_scaler(scaler_file_path)\n",
        "    station_data = load_station_data(data_file_path)\n",
        "\n",
        "    station_data_with_predictions = add_predictions_to_data(station_data, rf_model, scaler)\n",
        "\n",
        "    save_data_with_predictions(station_data_with_predictions, output_file_path_pred)"
      ],
      "metadata": {
        "id": "DWs_bxytCmZQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runPrediction()"
      ],
      "metadata": {
        "id": "ay7i2lgKEdut"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import time"
      ],
      "metadata": {
        "id": "9Mf12i7jFgbf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_batch():\n",
        "    # Read the JSON file into a DataFrame\n",
        "    df = spark.read.option(\"multiline\", \"true\").json(json_file_path)\n",
        "\n",
        "    # Create a new column maintenance_priority\n",
        "    df = df.withColumn(\n",
        "        \"maintenance_priority\",\n",
        "        (df[\"num_docks_disabled\"] + df[\"num_vehicles_disabled\"]) /\n",
        "        (df[\"num_docks_available\"] + df[\"num_vehicles_available\"])\n",
        "    )\n",
        "\n",
        "    # Order the DataFrame in descending order of maintenance_priority\n",
        "    df = df.orderBy(\"maintenance_priority\", ascending=False)\n",
        "\n",
        "    df.createOrReplaceTempView(\"station\")\n",
        "\n",
        "    try:\n",
        "\n",
        "        # Save the DataFrame as a JSON file\n",
        "        df.write.json(output_file_path_spark,mode=\"overwrite\")\n",
        "\n",
        "    finally:\n",
        "        # Unpersist the DataFrame to release resources\n",
        "        df.unpersist()"
      ],
      "metadata": {
        "id": "FVkXJCxhFnSr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_hour_global = datetime.now(eastern).hour\n",
        "print(current_hour_global)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsU9s4DzHgIo",
        "outputId": "9d4e8b1a-c7da-4a9b-b16e-a96ec92d4b60"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark session\n",
        "#spark = SparkSession.builder.appName(\"BatchApp\").getOrCreate()\n",
        "\n",
        "# Specify the path to the JSON file\n",
        "json_file_path = output_file_path_pred\n",
        "\n",
        "# Set the interval in seconds\n",
        "interval_seconds = 15\n",
        "\n",
        "try:\n",
        "    # Run indefinitely\n",
        "    while True:\n",
        "        # Record the start time for each iteration\n",
        "        iteration_start_time = time.time()\n",
        "\n",
        "        current_datetime = datetime.now(eastern)\n",
        "        if current_datetime.hour != current_hour_global:\n",
        "          runPrediction()\n",
        "          current_hour_global = current_datetime.hour\n",
        "\n",
        "        # Process the batch\n",
        "        process_batch()\n",
        "\n",
        "        # Calculate and print the time taken for the iteration\n",
        "        iteration_end_time = time.time()\n",
        "        iteration_elapsed_time = iteration_end_time - iteration_start_time\n",
        "        print(f\"Time taken for iteration: {iteration_elapsed_time:.2f} seconds\")\n",
        "\n",
        "        # Wait for the specified interval\n",
        "        time.sleep(interval_seconds)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    # Handle keyboard interrupt (e.g., press Ctrl+C to stop the loop)\n",
        "    print(\"Stopping the application\")\n",
        "\n",
        "finally:\n",
        "    # Stop the Spark session\n",
        "    spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "froX3zA-u14E",
        "outputId": "42548fca-412c-4433-9020-e7d276c8357b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken for iteration: 12.18 seconds\n",
            "Time taken for iteration: 3.10 seconds\n",
            "Time taken for iteration: 2.35 seconds\n",
            "Time taken for iteration: 1.50 seconds\n",
            "Time taken for iteration: 1.29 seconds\n",
            "Time taken for iteration: 1.37 seconds\n",
            "Time taken for iteration: 1.17 seconds\n",
            "Time taken for iteration: 1.92 seconds\n",
            "Time taken for iteration: 2.32 seconds\n",
            "Time taken for iteration: 1.93 seconds\n",
            "Stopping the application\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TozooPrcIrQE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}