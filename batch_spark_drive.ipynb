{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTmc1gcAmKdq",
        "outputId": "c2d83bae-b38d-4a9b-9bc8-f199a45425f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=22d6b44fcd261f9604564fc38f4710083d8c5de64b275a5a0a6d7a5061a45b79\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DuSn3YcmpwD",
        "outputId": "7e43dc13-ec6c-4c4b-b38e-378997960aa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "\n",
        "def process_batch():\n",
        "    # Read the JSON file into a DataFrame\n",
        "    df = spark.read.option(\"multiline\", \"true\").json(json_file_path)\n",
        "\n",
        "    # Create a new column maintenance_priority\n",
        "    df = df.withColumn(\n",
        "        \"maintenance_priority\",\n",
        "        (df[\"num_docks_disabled\"] + df[\"num_vehicles_disabled\"]) /\n",
        "        (df[\"num_docks_available\"] + df[\"num_vehicles_available\"])\n",
        "    )\n",
        "\n",
        "    # Order the DataFrame in descending order of maintenance_priority\n",
        "    df = df.orderBy(\"maintenance_priority\", ascending=False)\n",
        "\n",
        "    df.createOrReplaceTempView(\"station\")\n",
        "\n",
        "    try:\n",
        "\n",
        "        # Save the DataFrame as a JSON file\n",
        "        output_path = \"/gdrive/My Drive/output.json\"\n",
        "        df.write.json(output_path,mode=\"overwrite\")\n",
        "\n",
        "    finally:\n",
        "        # Unpersist the DataFrame to release resources\n",
        "        df.unpersist()\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"BatchApp\").getOrCreate()\n",
        "\n",
        "# Specify the path to the JSON file\n",
        "json_file_path = \"/gdrive/My Drive/station_data.json\"\n",
        "\n",
        "# Set the interval in seconds\n",
        "interval_seconds = 15\n",
        "\n",
        "try:\n",
        "    # Run indefinitely\n",
        "    while True:\n",
        "        # Record the start time for each iteration\n",
        "        iteration_start_time = time.time()\n",
        "\n",
        "        # Process the batch\n",
        "        process_batch()\n",
        "\n",
        "        # Calculate and print the time taken for the iteration\n",
        "        iteration_end_time = time.time()\n",
        "        iteration_elapsed_time = iteration_end_time - iteration_start_time\n",
        "        print(f\"Time taken for iteration: {iteration_elapsed_time:.2f} seconds\")\n",
        "\n",
        "        # Wait for the specified interval\n",
        "        time.sleep(interval_seconds)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    # Handle keyboard interrupt (e.g., press Ctrl+C to stop the loop)\n",
        "    print(\"Stopping the application\")\n",
        "\n",
        "finally:\n",
        "    # Stop the Spark session\n",
        "    spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "froX3zA-u14E",
        "outputId": "82a59a34-d0e1-4ec0-a4a4-31168d6d3be7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken for iteration: 1.64 seconds\n",
            "Time taken for iteration: 0.99 seconds\n",
            "Stopping the application\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_master = spark.conf.get(\"spark.master\")\n",
        "print(spark_master)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdNggX2L0DXo",
        "outputId": "41ea2048-ffe5-4563-fae4-df338a634e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "local[*]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cja5uLLR3nzr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}