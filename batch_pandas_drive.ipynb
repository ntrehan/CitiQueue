{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DuSn3YcmpwD",
        "outputId": "c5ba3718-4e21-453a-f30c-89aa274b768b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "import time"
      ],
      "metadata": {
        "id": "UMWepRMQ_cuS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_file_path = '/gdrive/My Drive/trip_model.pkl'\n",
        "scaler_file_path = '/gdrive/My Drive/scaler_model.pkl'\n",
        "data_file_path = '/gdrive/My Drive/station_data.json'\n",
        "output_file_path_pred = '/gdrive/My Drive/station_data.json'\n",
        "output_file_path_pandas = \"/gdrive/My Drive/output_priority.json\""
      ],
      "metadata": {
        "id": "T08P2EJ7Dq9X"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_current_datetime():\n",
        "    eastern = pytz.timezone('US/Eastern')\n",
        "    return datetime.now(eastern).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "eastern = pytz.timezone('US/Eastern')"
      ],
      "metadata": {
        "id": "LDljfbbwHAmc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(file_path):\n",
        "    with open(file_path, 'rb') as model_file:\n",
        "        return pickle.load(model_file)\n",
        "\n",
        "def load_scaler(file_path):\n",
        "    with open(file_path, 'rb') as scaler_file:\n",
        "        return pickle.load(scaler_file)\n",
        "\n",
        "def load_station_data(file_path):\n",
        "    with open(file_path, 'r') as data_file:\n",
        "        return json.load(data_file)\n",
        "\n",
        "def scale_features(data_point, scaler):\n",
        "    return scaler.transform([data_point])[0]\n",
        "\n",
        "def predict_traffic(model, scaled_data_point):\n",
        "    return model.predict([scaled_data_point])[0]\n",
        "\n",
        "\n",
        "def add_predictions_to_data(station_data, model, scaler):\n",
        "    # Convert the station_data list of dictionaries to a pandas DataFrame\n",
        "    df = pd.DataFrame(station_data)\n",
        "\n",
        "    # Extract features into a separate DataFrame and rename columns\n",
        "    features = df[['day_of_week', 'hour', 'local_id']]\n",
        "    features = features.rename(columns={'day_of_week': 'day', 'hour': 'hour', 'local_id': 'station_id'})\n",
        "\n",
        "    # Scale the features using the loaded scaler\n",
        "    scaled_features = scaler.transform(features)\n",
        "\n",
        "    # Use the trained model to predict the target variable\n",
        "    df['predicted_traffic'] = model.predict(scaled_features)\n",
        "\n",
        "    # Convert the DataFrame back to a list of dictionaries\n",
        "    updated_station_data = df.to_dict('records')\n",
        "\n",
        "    return updated_station_data\n",
        "\n",
        "\n",
        "def save_data_with_predictions(station_data, output_file_path):\n",
        "    with open(output_file_path, 'w') as output_file:\n",
        "        json.dump(station_data, output_file)"
      ],
      "metadata": {
        "id": "aOkR0mQ4_fId"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def runPrediction():\n",
        "    rf_model = load_model(model_file_path)\n",
        "    scaler = load_scaler(scaler_file_path)\n",
        "    station_data = load_station_data(data_file_path)\n",
        "\n",
        "    station_data_with_predictions = add_predictions_to_data(station_data, rf_model, scaler)\n",
        "\n",
        "    save_data_with_predictions(station_data_with_predictions, output_file_path_pred)"
      ],
      "metadata": {
        "id": "DWs_bxytCmZQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runPrediction()"
      ],
      "metadata": {
        "id": "ay7i2lgKEdut"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_batch():\n",
        "    # Read the JSON file into a Pandas DataFrame\n",
        "    df = pd.read_json(output_file_path_pred)\n",
        "\n",
        "    # Create a new column maintenance_priority\n",
        "    df[\"maintenance_priority\"] = (df[\"num_docks_disabled\"] + df[\"num_vehicles_disabled\"]) / \\\n",
        "                                 (df[\"num_docks_available\"] + df[\"num_vehicles_available\"])\n",
        "\n",
        "    # Order the DataFrame in descending order of maintenance_priority\n",
        "    df = df.sort_values(by=\"maintenance_priority\", ascending=False)\n",
        "\n",
        "    df.to_json(output_file_path_pandas, orient=\"records\", lines=True)"
      ],
      "metadata": {
        "id": "FVkXJCxhFnSr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json(output_file_path_pred)"
      ],
      "metadata": {
        "id": "K4JnGKijRdLG"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_hour_global = datetime.now(eastern).hour\n",
        "print(current_hour_global)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsU9s4DzHgIo",
        "outputId": "ddbc8d06-df4f-48b3-f099-f7ba7df0ef60"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the interval in seconds\n",
        "interval_seconds = 15\n",
        "\n",
        "try:\n",
        "    # Run indefinitely\n",
        "    while True:\n",
        "        # Record the start time for each iteration\n",
        "        iteration_start_time = time.time()\n",
        "\n",
        "        current_datetime = datetime.now(eastern)\n",
        "        if current_datetime.hour != current_hour_global:\n",
        "          runPrediction()\n",
        "          current_hour_global = current_datetime.hour\n",
        "\n",
        "        # Process the batch\n",
        "        process_batch()\n",
        "\n",
        "        # Calculate and print the time taken for the iteration\n",
        "        iteration_end_time = time.time()\n",
        "        iteration_elapsed_time = iteration_end_time - iteration_start_time\n",
        "        print(f\"Time taken for iteration: {iteration_elapsed_time:.2f} seconds\")\n",
        "\n",
        "        # Wait for the specified interval\n",
        "        time.sleep(interval_seconds)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    # Handle keyboard interrupt (e.g., press Ctrl+C to stop the loop)\n",
        "    print(\"Stopping the application\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "froX3zA-u14E",
        "outputId": "8117f0f2-1b6b-4b2d-a32b-a32eefd1e555"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken for iteration: 12.23 seconds\n",
            "Time taken for iteration: 0.06 seconds\n",
            "Time taken for iteration: 0.06 seconds\n",
            "Time taken for iteration: 0.05 seconds\n",
            "Time taken for iteration: 0.09 seconds\n",
            "Time taken for iteration: 0.05 seconds\n",
            "Time taken for iteration: 0.05 seconds\n",
            "Time taken for iteration: 0.05 seconds\n",
            "Time taken for iteration: 0.08 seconds\n",
            "Time taken for iteration: 0.08 seconds\n",
            "Time taken for iteration: 0.08 seconds\n",
            "Time taken for iteration: 0.08 seconds\n",
            "Time taken for iteration: 0.06 seconds\n",
            "Time taken for iteration: 0.05 seconds\n",
            "Time taken for iteration: 0.05 seconds\n",
            "Time taken for iteration: 0.05 seconds\n",
            "Time taken for iteration: 0.06 seconds\n",
            "Time taken for iteration: 0.06 seconds\n",
            "Stopping the application\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TozooPrcIrQE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}